{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaaa483c-a453-4baa-a334-9fff984ad426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train processing...\n",
      "test processing...\n",
      "ok!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import scipy.signal as sg\n",
    "import wfdb\n",
    "\n",
    "PATH = Path(\"dataset\")\n",
    "sampling_rate = 360\n",
    "\n",
    "# non-beat labels\n",
    "invalid_labels = ['|', '~', '!', '+', '[', ']', '\"', 'x']\n",
    "\n",
    "# for correct R-peak location\n",
    "tol = 0.05\n",
    "\n",
    "\n",
    "def worker(record):\n",
    "    # read ML II signal & r-peaks position and labels\n",
    "    signal = wfdb.rdrecord((PATH / record).as_posix(), channels=[0]).p_signal[:, 0]\n",
    "\n",
    "    annotation = wfdb.rdann((PATH / record).as_posix(), extension=\"atr\")\n",
    "    r_peaks, labels = annotation.sample, np.array(annotation.symbol)\n",
    "\n",
    "    # filtering uses a 200-ms width median filter and 600-ms width median filter\n",
    "    baseline = sg.medfilt(sg.medfilt(signal, int(0.2 * sampling_rate) - 1), int(0.6 * sampling_rate) - 1)\n",
    "    filtered_signal = signal - baseline\n",
    "\n",
    "    # remove non-beat labels\n",
    "    indices = [i for i, label in enumerate(labels) if label not in invalid_labels]\n",
    "    r_peaks, labels = r_peaks[indices], labels[indices]\n",
    "\n",
    "    # align r-peaks\n",
    "    newR = []\n",
    "    for r_peak in r_peaks:\n",
    "        r_left = np.maximum(r_peak - int(tol * sampling_rate), 0)\n",
    "        r_right = np.minimum(r_peak + int(tol * sampling_rate), len(filtered_signal))\n",
    "        newR.append(r_left + np.argmax(filtered_signal[r_left:r_right]))\n",
    "    r_peaks = np.array(newR, dtype=\"int\")\n",
    "\n",
    "    # remove inter-patient variation\n",
    "    normalized_signal = filtered_signal / np.mean(filtered_signal[r_peaks])\n",
    "\n",
    "    # AAMI categories\n",
    "    AAMI = {\n",
    "        \"N\": 0, \"L\": 0, \"R\": 0, \"e\": 0, \"j\": 0,  # N\n",
    "        \"A\": 1, \"a\": 1, \"S\": 1, \"J\": 1,  # SVEB\n",
    "        \"V\": 2, \"E\": 2,  # VEB\n",
    "        \"F\": 3,  # F\n",
    "        \"/\": 4, \"f\": 4, \"Q\": 4  # Q\n",
    "    }\n",
    "    categories = [AAMI[label] for label in labels]\n",
    "\n",
    "    return {\n",
    "        \"record\": record,\n",
    "        \"signal\": normalized_signal, \"r_peaks\": r_peaks, \"categories\": categories\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # for multi-processing\n",
    "    cpus = 22 if joblib.cpu_count() > 22 else joblib.cpu_count() - 1\n",
    "\n",
    "    train_records = [\n",
    "        '101', '106', '108', '109', '112', '114', '115', '116', '118', '119',\n",
    "        '122', '124', '201', '203', '205', '207', '208', '209', '215', '220',\n",
    "        '223', '230'\n",
    "    ]\n",
    "    print(\"train processing...\")\n",
    "    with ProcessPoolExecutor(max_workers=cpus) as executor:\n",
    "        train_data = [result for result in executor.map(worker, train_records)]\n",
    "\n",
    "    test_records = [\n",
    "        '100', '103', '105', '111', '113', '117', '121', '123', '200', '202',\n",
    "        '210', '212', '213', '214', '219', '221', '222', '228', '231', '232',\n",
    "        '233', '234'\n",
    "    ]\n",
    "    print(\"test processing...\")\n",
    "    with ProcessPoolExecutor(max_workers=cpus) as executor:\n",
    "        test_data = [result for result in executor.map(worker, test_records)]\n",
    "\n",
    "    with open((PATH / \"mitdb.pkl\").as_posix(), \"wb\") as f:\n",
    "        pickle.dump((train_data, test_data), f, protocol=4)\n",
    "\n",
    "    print(\"ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89711e71-aa44-4b41-a30a-3d7c06410257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import cv2\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pywt\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from functools import partial\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorer\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EpochScoring, Initializer, LRScheduler, TensorBoard\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import predefined_split\n",
    "from torch.backends import cudnn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 7)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pooling1 = nn.MaxPool2d(5)\n",
    "        self.pooling2 = nn.MaxPool2d(3)\n",
    "        self.pooling3 = nn.AdaptiveMaxPool2d((1, 1))\n",
    "        self.fc1 = nn.Linear(68, 32)\n",
    "        self.fc2 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.bn1(self.conv1(x1)))  # (16 x 94 x 94)\n",
    "        x1 = self.pooling1(x1)  # (16 x 18 x 18)\n",
    "        x1 = F.relu(self.bn2(self.conv2(x1)))  # (32 x 16 x 16)\n",
    "        x1 = self.pooling2(x1)  # (32 x 5 x 5)\n",
    "        x1 = F.relu(self.bn3(self.conv3(x1)))  # (64 x 3 x 3)\n",
    "        x1 = self.pooling3(x1)  # (64 x 1 x 1)\n",
    "        x1 = x1.view((-1, 64))  # (64,)\n",
    "        x = torch.cat((x1, x2), dim=1)  # (68,)\n",
    "        x = F.relu(self.fc1(x))  # (32,)\n",
    "        x = self.fc2(x)  # (4,)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec64cf22-7d97-444a-8574-3b1897bb89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(data, wavelet, scales, sampling_period):\n",
    "    # heartbeat segmentation interval\n",
    "    before, after = 90, 110\n",
    "\n",
    "    coeffs, frequencies = pywt.cwt(data[\"signal\"], scales, wavelet, sampling_period)\n",
    "    r_peaks, categories = data[\"r_peaks\"], data[\"categories\"]\n",
    "\n",
    "    # for remove inter-patient variation\n",
    "    avg_rri = np.mean(np.diff(r_peaks))\n",
    "\n",
    "    x1, x2, y, groups = [], [], [], []\n",
    "    for i in range(len(r_peaks)):\n",
    "        if i == 0 or i == len(r_peaks) - 1:\n",
    "            continue\n",
    "\n",
    "        if categories[i] == 4:  # remove AAMI Q class\n",
    "            continue\n",
    "\n",
    "        # cv2.resize is used to sampling the scalogram to (100 x100)\n",
    "        x1.append(cv2.resize(coeffs[:, r_peaks[i] - before: r_peaks[i] + after], (100, 100)))\n",
    "        x2.append([\n",
    "            r_peaks[i] - r_peaks[i - 1] - avg_rri,  # previous RR Interval\n",
    "            r_peaks[i + 1] - r_peaks[i] - avg_rri,  # post RR Interval\n",
    "            (r_peaks[i] - r_peaks[i - 1]) / (r_peaks[i + 1] - r_peaks[i]),  # ratio RR Interval\n",
    "            np.mean(np.diff(r_peaks[np.maximum(i - 10, 0):i + 1])) - avg_rri  # local RR Interval\n",
    "        ])\n",
    "        y.append(categories[i])\n",
    "        groups.append(data[\"record\"])\n",
    "\n",
    "    return x1, x2, y, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f59961-6ac3-4e0a-98eb-f2247e2b3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(wavelet, scales, sampling_rate, filename=\"./dataset/mitdb.pkl\"):\n",
    "    import pickle\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        train_data, test_data = pickle.load(f)\n",
    "\n",
    "    cpus = 22 if joblib.cpu_count() > 22 else joblib.cpu_count() - 1  # for multi-process\n",
    "\n",
    "    # for training\n",
    "    x1_train, x2_train, y_train, groups_train = [], [], [], []\n",
    "    with ProcessPoolExecutor(max_workers=cpus) as executor:\n",
    "        for x1, x2, y, groups in executor.map(\n",
    "                partial(worker, wavelet=wavelet, scales=scales, sampling_period=1. / sampling_rate), train_data):\n",
    "            x1_train.append(x1)\n",
    "            x2_train.append(x2)\n",
    "            y_train.append(y)\n",
    "            groups_train.append(groups)\n",
    "\n",
    "    x1_train = np.expand_dims(np.concatenate(x1_train, axis=0), axis=1).astype(np.float32)\n",
    "    x2_train = np.concatenate(x2_train, axis=0).astype(np.float32)\n",
    "    y_train = np.concatenate(y_train, axis=0).astype(np.int64)\n",
    "    groups_train = np.concatenate(groups_train, axis=0)\n",
    "\n",
    "    # for test\n",
    "    x1_test, x2_test, y_test, groups_test = [], [], [], []\n",
    "    with ProcessPoolExecutor(max_workers=cpus) as executor:\n",
    "        for x1, x2, y, groups in executor.map(\n",
    "                partial(worker, wavelet=wavelet, scales=scales, sampling_period=1. / sampling_rate), test_data):\n",
    "            x1_test.append(x1)\n",
    "            x2_test.append(x2)\n",
    "            y_test.append(y)\n",
    "            groups_test.append(groups)\n",
    "\n",
    "    x1_test = np.expand_dims(np.concatenate(x1_test, axis=0), axis=1).astype(np.float32)\n",
    "    x2_test = np.concatenate(x2_test, axis=0).astype(np.float32)\n",
    "    y_test = np.concatenate(y_test, axis=0).astype(np.int64)\n",
    "    groups_test = np.concatenate(groups_test, axis=0)\n",
    "\n",
    "    # normalization\n",
    "    scaler = RobustScaler()\n",
    "    x2_train = scaler.fit_transform(x2_train)\n",
    "    x2_test = scaler.transform(x2_test)\n",
    "\n",
    "    return (x1_train, x2_train, y_train, groups_train), (x1_test, x2_test, y_test, groups_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "314d3bb8-b998-4f48-bc86-b226d23d58a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "  epoch    train_loss    valid_acc    valid_f1    valid_loss      lr     dur\n",
      "-------  ------------  -----------  ----------  ------------  ------  ------\n",
      "      1        \u001b[36m0.3160\u001b[0m       \u001b[32m0.9467\u001b[0m      \u001b[35m0.4657\u001b[0m        \u001b[31m0.2194\u001b[0m  0.0010  9.3147\n",
      "      2        \u001b[36m0.1528\u001b[0m       \u001b[32m0.9560\u001b[0m      \u001b[35m0.5589\u001b[0m        \u001b[31m0.1662\u001b[0m  0.0010  9.2797\n",
      "      3        \u001b[36m0.1049\u001b[0m       0.9549      0.5542        \u001b[31m0.1514\u001b[0m  0.0010  9.3954\n",
      "      4        \u001b[36m0.0814\u001b[0m       \u001b[32m0.9655\u001b[0m      \u001b[35m0.6425\u001b[0m        \u001b[31m0.1340\u001b[0m  0.0010  9.2854\n",
      "      5        \u001b[36m0.0666\u001b[0m       \u001b[32m0.9721\u001b[0m      \u001b[35m0.6842\u001b[0m        0.1418  0.0010  9.1652\n",
      "      6        \u001b[36m0.0560\u001b[0m       \u001b[32m0.9734\u001b[0m      \u001b[35m0.6875\u001b[0m        \u001b[31m0.1317\u001b[0m  0.0001  9.7755\n",
      "      7        \u001b[36m0.0540\u001b[0m       0.9732      \u001b[35m0.6882\u001b[0m        0.1342  0.0001  9.5134\n",
      "      8        \u001b[36m0.0526\u001b[0m       \u001b[32m0.9738\u001b[0m      \u001b[35m0.6896\u001b[0m        \u001b[31m0.1295\u001b[0m  0.0001  9.0133\n",
      "      9        \u001b[36m0.0515\u001b[0m       \u001b[32m0.9739\u001b[0m      \u001b[35m0.6901\u001b[0m        0.1303  0.0001  9.4497\n",
      "     10        \u001b[36m0.0501\u001b[0m       0.9734      0.6893        0.1318  0.0001  9.5966\n",
      "     11        \u001b[36m0.0491\u001b[0m       0.9739      \u001b[35m0.6902\u001b[0m        \u001b[31m0.1293\u001b[0m  0.0000  9.2373\n",
      "     12        \u001b[36m0.0489\u001b[0m       0.9735      0.6893        0.1318  0.0000  9.5675\n",
      "     13        \u001b[36m0.0488\u001b[0m       0.9735      0.6894        0.1314  0.0000  9.3591\n",
      "     14        \u001b[36m0.0486\u001b[0m       0.9736      0.6896        0.1311  0.0000  9.2979\n",
      "     15        0.0487       0.9738      0.6898        0.1301  0.0000  9.3720\n",
      "     16        \u001b[36m0.0484\u001b[0m       0.9738      0.6901        0.1302  0.0000  9.5726\n",
      "     17        0.0484       0.9738      0.6901        0.1303  0.0000  9.9885\n",
      "     18        0.0485       0.9738      0.6901        0.1299  0.0000  9.1080\n",
      "     19        \u001b[36m0.0483\u001b[0m       0.9737      0.6898        0.1301  0.0000  9.1622\n",
      "     20        0.0484       0.9738      0.6898        0.1301  0.0000  9.6274\n",
      "     21        0.0485       0.9736      0.6897        0.1308  0.0000  9.3578\n",
      "     22        \u001b[36m0.0481\u001b[0m       0.9738      0.6902        0.1302  0.0000  9.4223\n",
      "     23        0.0483       0.9737      0.6899        0.1305  0.0000  9.4724\n",
      "     24        0.0483       0.9733      0.6889        0.1318  0.0000  9.3538\n",
      "     25        0.0484       0.9736      0.6898        0.1312  0.0000  9.8805\n",
      "     26        0.0484       0.9735      0.6898        0.1318  0.0000  9.1682\n",
      "     27        0.0485       0.9737      0.6898        0.1306  0.0000  9.0572\n",
      "     28        0.0483       0.9737      0.6896        0.1306  0.0000  9.0535\n",
      "     29        0.0488       \u001b[32m0.9739\u001b[0m      \u001b[35m0.6902\u001b[0m        0.1294  0.0000  9.0220\n",
      "     30        0.0482       0.9735      0.6894        0.1315  0.0000  9.3271\n",
      "Training time: 287.02390360832214s\n",
      "[[43735   356    92    35]\n",
      " [  231  1530    66     9]\n",
      " [  131     6  3078     4]\n",
      " [  365     0    23     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9836    0.9891    0.9864     44218\n",
      "           1     0.8087    0.8333    0.8208      1836\n",
      "           2     0.9445    0.9562    0.9503      3219\n",
      "           3     0.0000    0.0000    0.0000       388\n",
      "\n",
      "    accuracy                         0.9735     49661\n",
      "   macro avg     0.6842    0.6947    0.6894     49661\n",
      "weighted avg     0.9670    0.9735    0.9702     49661\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def main():\n",
    "    sampling_rate = 360\n",
    "\n",
    "    wavelet = \"mexh\"  # mexh, morl, gaus8, gaus4\n",
    "    scales = pywt.central_frequency(wavelet) * sampling_rate / np.arange(1, 101, 1)\n",
    "\n",
    "    (x1_train, x2_train, y_train, groups_train), (x1_test, x2_test, y_test, groups_test) = load_data(\n",
    "        wavelet=wavelet, scales=scales, sampling_rate=sampling_rate)\n",
    "    print(\"Data loaded successfully!\")\n",
    "\n",
    "    log_dir = \"./logs/{}\".format(wavelet)\n",
    "    shutil.rmtree(log_dir, ignore_errors=True)\n",
    "\n",
    "    callbacks = [\n",
    "        Initializer(\"[conv|fc]*.weight\", fn=torch.nn.init.kaiming_normal_),\n",
    "        Initializer(\"[conv|fc]*.bias\", fn=partial(torch.nn.init.constant_, val=0.0)),\n",
    "        LRScheduler(policy=StepLR, step_size=5, gamma=0.1),\n",
    "        EpochScoring(scoring=make_scorer(f1_score, average=\"macro\"), lower_is_better=False, name=\"valid_f1\"),\n",
    "        TensorBoard(SummaryWriter(log_dir))\n",
    "    ]\n",
    "    net = NeuralNetClassifier(  # skorch is extensive package of pytorch for compatible with scikit-learn\n",
    "        MyModule,\n",
    "        criterion=torch.nn.CrossEntropyLoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        lr=0.001,\n",
    "        max_epochs=30,\n",
    "        batch_size=1024,\n",
    "        train_split=predefined_split(Dataset({\"x1\": x1_test, \"x2\": x2_test}, y_test)),\n",
    "        verbose=1,\n",
    "        device=\"cuda\",\n",
    "        callbacks=callbacks,\n",
    "        iterator_train__shuffle=True,\n",
    "        optimizer__weight_decay=0,\n",
    "    )\n",
    "    start = time.time()\n",
    "    net.fit({\"x1\": x1_train, \"x2\": x2_train}, y_train)\n",
    "    stop = time.time()\n",
    "    print(f\"Training time: {stop - start}s\")\n",
    "    y_true, y_pred = y_test, net.predict({\"x1\": x1_test, \"x2\": x2_test})\n",
    "\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
